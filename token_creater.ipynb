{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Prakhar Pande\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Prakhar Pande\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Prakhar Pande\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Prakhar Pande\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Prakhar Pande\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Prakhar Pande\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n"
     ]
    }
   ],
   "source": [
    "# load training dataset (6K)\n",
    "filename = 'image_captioning/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions: train=6000\n"
     ]
    }
   ],
   "source": [
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
